{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9905b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2288ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_undersampled_train_df = pd.read_csv(os.path.join('./Data', \"extracted_features_undersampled_train_data.csv\"))\n",
    "extracted_features_oversampled_train_df = pd.read_csv(os.path.join('./Data', \"extracted_features_oversampled_train_data.csv\"))\n",
    "extracted_features_train_df = pd.read_csv(os.path.join('./Data', \"extracted_features_train_data.csv\"))\n",
    "train_df = pd.read_csv(os.path.join('./Data', \"train_data.csv\"))\n",
    "test_df = pd.read_csv(os.path.join('./Data', \"test_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "62ea7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = ['V3', 'V4', 'V7', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18']\n",
    "X_test = test_df[target_features].values\n",
    "y_test = test_df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d0fad5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"--- {name} ---\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8773da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(models_proba, y_test):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for name, y_proba in models_proba.items():\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Comparison of ROC Curves')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb8be2",
   "metadata": {},
   "source": [
    "# Oversampled Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d483804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = extracted_features_oversampled_train_df.drop('Class', axis=1).values\n",
    "y_train = extracted_features_oversampled_train_df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4067a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(\n",
    "    criterion='gini',       \n",
    "    max_depth=15,           # control tree depth to prevent overfitting\n",
    "    min_samples_split=50,   # minimum samples required to split an internal node\n",
    "    min_samples_leaf=20,    # minimum samples required to be at a leaf node\n",
    "    random_state=42         \n",
    ")\n",
    "oversampled_dtree_model = dtree.fit(X_train, y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=9,        # Number of neighbors\n",
    "    weights='distance',    # 'uniform' or 'distance'\n",
    "    algorithm='auto',     # Search algorithm\n",
    "    p=2                   # 2 for Euclidean distance\n",
    ")\n",
    "oversampled_knn_model = knn.fit(X_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,       # Number of boosting rounds\n",
    "    max_depth=10,            # Maximum tree depth\n",
    "    learning_rate=0.1,      # Step size shrinkage\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "oversampled_xgb_model = xgb.fit(X_train, y_train)\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty='l2',         # Regularization type\n",
    "    C=0.001,                # Inverse regularization strength\n",
    "    solver='liblinear',   # Solver for small datasets\n",
    "    random_state=42\n",
    ")\n",
    "oversampled_log_reg_model = log_reg.fit(X_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,       # Number of trees\n",
    "    max_depth=5,           # Max depth\n",
    "    min_samples_split=50,   # Minimum samples to split\n",
    "    min_samples_leaf=50,    # Minimum samples at leaf\n",
    "    random_state=42\n",
    ")\n",
    "oversampled_rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "91ad75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     56864\n",
      "           1       0.12      0.85      0.21        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.56      0.92      0.60     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56246   618]\n",
      " [   15    83]]\n",
      "--- KNN ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     56864\n",
      "           1       0.22      0.89      0.36        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.61      0.94      0.68     56962\n",
      "weighted avg       1.00      0.99      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56559   305]\n",
      " [   11    87]]\n",
      "--- XGBoost ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.48      0.89      0.62        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.74      0.94      0.81     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56770    94]\n",
      " [   11    87]]\n",
      "--- Logistic Regression ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     56864\n",
      "           1       0.05      0.92      0.10        98\n",
      "\n",
      "    accuracy                           0.97     56962\n",
      "   macro avg       0.53      0.94      0.54     56962\n",
      "weighted avg       1.00      0.97      0.98     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[55249  1615]\n",
      " [    8    90]]\n",
      "--- Random Forest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     56864\n",
      "           1       0.08      0.90      0.15        98\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.54      0.94      0.57     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[55897   967]\n",
      " [   10    88]]\n"
     ]
    }
   ],
   "source": [
    "# Apply on all models\n",
    "test_model(\"Decision Tree\", oversampled_dtree_model, X_test, y_test)\n",
    "test_model(\"KNN\", oversampled_knn_model, X_test, y_test)\n",
    "test_model(\"XGBoost\", oversampled_xgb_model, X_test, y_test)\n",
    "test_model(\"Logistic Regression\", oversampled_log_reg_model, X_test, y_test)\n",
    "test_model(\"Random Forest\", oversampled_rf_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba037b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_proba = {\n",
    "    \"Logistic Regression\": oversampled_log_reg_model.predict_proba(X_test)[:, 1],\n",
    "    \"Random Forest\": oversampled_rf_model.predict_proba(X_test)[:, 1],\n",
    "    \"Decision Tree\": oversampled_dtree_model.predict_proba(X_test)[:, 1],\n",
    "    \"KNN\": oversampled_knn_model.predict_proba(X_test)[:, 1],\n",
    "    \"XGBoost\": oversampled_xgb_model.predict_proba(X_test)[:, 1]\n",
    "}\n",
    "plot_roc_curves(models_proba, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c9719",
   "metadata": {},
   "source": [
    "# Undersampled Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "254ff210",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = extracted_features_undersampled_train_df.drop('Class', axis=1).values\n",
    "y_train = extracted_features_undersampled_train_df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ceb8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(\n",
    "    criterion='gini',       \n",
    "    max_depth=15,           # control tree depth to prevent overfitting\n",
    "    min_samples_split=50,   # minimum samples required to split an internal node\n",
    "    min_samples_leaf=10,    # minimum samples required to be at a leaf node\n",
    "    random_state=42         \n",
    ")\n",
    "undersampled_dtree_model = dtree.fit(X_train, y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=7,        # Number of neighbors\n",
    "    weights='distance',    # 'uniform' or 'distance'\n",
    "    algorithm='auto',     # Search algorithm\n",
    "    p=2                   # 2 for Euclidean distance\n",
    ")\n",
    "undersampled_knn_model = knn.fit(X_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,       # Number of boosting rounds\n",
    "    max_depth=10,            # Maximum tree depth\n",
    "    learning_rate=0.1,      # Step size shrinkage\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "undersampled_xgb_model = xgb.fit(X_train, y_train)\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty='l2',         # Regularization type\n",
    "    C=0.01,                # Inverse regularization strength\n",
    "    solver='liblinear',   # Solver for small datasets\n",
    "    random_state=42\n",
    ")\n",
    "undersampled_log_reg_model = log_reg.fit(X_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,       # Number of trees\n",
    "    max_depth=5,           # Max depth\n",
    "    min_samples_split=50,   # Minimum samples to split\n",
    "    min_samples_leaf=50,    # Minimum samples at leaf\n",
    "    random_state=42\n",
    ")\n",
    "undersampled_rf_model = rf.fit(X_train, y_train)\n",
    "\n",
    "svm = SVC(\n",
    "    kernel='linear',        # Kernel type: 'linear', 'poly', 'rbf', 'sigmoid'\n",
    "    C=10,               # Regularization parameter\n",
    "    gamma='scale',       # Kernel coefficient\n",
    "    probability=True,    # Enable probability estimates if needed\n",
    "    random_state=42\n",
    ")\n",
    "undersampled_svm_model = svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1e98aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     56864\n",
      "           1       0.03      0.90      0.05        98\n",
      "\n",
      "    accuracy                           0.95     56962\n",
      "   macro avg       0.51      0.92      0.51     56962\n",
      "weighted avg       1.00      0.95      0.97     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[53754  3110]\n",
      " [   10    88]]\n",
      "--- KNN ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     56864\n",
      "           1       0.05      0.91      0.10        98\n",
      "\n",
      "    accuracy                           0.97     56962\n",
      "   macro avg       0.53      0.94      0.54     56962\n",
      "weighted avg       1.00      0.97      0.98     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[55287  1577]\n",
      " [    9    89]]\n",
      "--- XGBoost ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98     56864\n",
      "           1       0.03      0.93      0.06        98\n",
      "\n",
      "    accuracy                           0.95     56962\n",
      "   macro avg       0.52      0.94      0.52     56962\n",
      "weighted avg       1.00      0.95      0.97     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[54108  2756]\n",
      " [    7    91]]\n",
      "--- Logistic Regression ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     56864\n",
      "           1       0.03      0.93      0.07        98\n",
      "\n",
      "    accuracy                           0.96     56962\n",
      "   macro avg       0.52      0.94      0.52     56962\n",
      "weighted avg       1.00      0.96      0.98     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[54348  2516]\n",
      " [    7    91]]\n",
      "--- Random Forest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     56864\n",
      "           1       0.05      0.89      0.09        98\n",
      "\n",
      "    accuracy                           0.97     56962\n",
      "   macro avg       0.52      0.93      0.54     56962\n",
      "weighted avg       1.00      0.97      0.98     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[55185  1679]\n",
      " [   11    87]]\n",
      "--- SVM ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     56864\n",
      "           1       0.05      0.92      0.10        98\n",
      "\n",
      "    accuracy                           0.97     56962\n",
      "   macro avg       0.53      0.94      0.54     56962\n",
      "weighted avg       1.00      0.97      0.98     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[55234  1630]\n",
      " [    8    90]]\n"
     ]
    }
   ],
   "source": [
    "# Apply on all models\n",
    "test_model(\"Decision Tree\", undersampled_dtree_model, X_test, y_test)\n",
    "test_model(\"KNN\", undersampled_knn_model, X_test, y_test)\n",
    "test_model(\"XGBoost\", undersampled_xgb_model, X_test, y_test)\n",
    "test_model(\"Logistic Regression\", undersampled_log_reg_model, X_test, y_test)\n",
    "test_model(\"Random Forest\", undersampled_rf_model, X_test, y_test)\n",
    "test_model(\"SVM\", undersampled_svm_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_proba = {\n",
    "    \"Logistic Regression\": undersampled_log_reg_model.predict_proba(X_test)[:, 1],\n",
    "    \"Random Forest\": undersampled_rf_model.predict_proba(X_test)[:, 1],\n",
    "    \"Decision Tree\": undersampled_dtree_model.predict_proba(X_test)[:, 1],\n",
    "    \"KNN\": undersampled_knn_model.predict_proba(X_test)[:, 1],\n",
    "    \"XGBoost\": undersampled_xgb_model.predict_proba(X_test)[:, 1],\n",
    "    \"SVM\": undersampled_svm_model.predict_proba(X_test)[:, 1]\n",
    "}\n",
    "plot_roc_curves(models_proba, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636d034",
   "metadata": {},
   "source": [
    "# Original Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf550062",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = extracted_features_train_df.drop('Class', axis=1).values\n",
    "y_train = extracted_features_train_df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc70eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(\n",
    "    criterion='gini',       \n",
    "    max_depth=15,           # control tree depth to prevent overfitting\n",
    "    min_samples_split=50,   # minimum samples required to split an internal node\n",
    "    min_samples_leaf=20,    # minimum samples required to be at a leaf node\n",
    "    random_state=42         \n",
    ")\n",
    "dtree_model = dtree.fit(X_train, y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=9,        # Number of neighbors\n",
    "    weights='distance',    # 'uniform' or 'distance'\n",
    "    algorithm='auto',     # Search algorithm\n",
    "    p=2                   # 2 for Euclidean distance\n",
    ")\n",
    "knn_model = knn.fit(X_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,       # Number of boosting rounds\n",
    "    max_depth=10,            # Maximum tree depth\n",
    "    learning_rate=0.1,      # Step size shrinkage\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model = xgb.fit(X_train, y_train)\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty='l2',         # Regularization type\n",
    "    C=0.001,                # Inverse regularization strength\n",
    "    solver='liblinear',   # Solver for small datasets\n",
    "    random_state=42\n",
    ")\n",
    "log_reg_model = log_reg.fit(X_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,       # Number of trees\n",
    "    max_depth=5,           # Max depth\n",
    "    min_samples_split=50,   # Minimum samples to split\n",
    "    min_samples_leaf=50,    # Minimum samples at leaf\n",
    "    random_state=42\n",
    ")\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "07697290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.71      0.82      0.76        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.86      0.91      0.88     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56832    32]\n",
      " [   18    80]]\n",
      "--- KNN ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.71      0.88      0.79        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.86      0.94      0.89     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56829    35]\n",
      " [   12    86]]\n",
      "--- XGBoost ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.71      0.83      0.76        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.86      0.91      0.88     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56831    33]\n",
      " [   17    81]]\n",
      "--- Logistic Regression ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.83      0.76      0.79        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.92      0.88      0.90     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56849    15]\n",
      " [   24    74]]\n",
      "--- Random Forest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.80      0.76      0.78        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.90      0.88      0.89     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56846    18]\n",
      " [   24    74]]\n"
     ]
    }
   ],
   "source": [
    "# Apply on all models\n",
    "test_model(\"Decision Tree\", dtree_model, X_test, y_test)\n",
    "test_model(\"KNN\", knn_model, X_test, y_test)\n",
    "test_model(\"XGBoost\", xgb_model, X_test, y_test)\n",
    "test_model(\"Logistic Regression\", log_reg_model, X_test, y_test)\n",
    "test_model(\"Random Forest\", rf_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4023e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_proba = {\n",
    "    \"Logistic Regression\": log_reg_model.predict_proba(X_test)[:, 1],\n",
    "    \"Random Forest\": rf_model.predict_proba(X_test)[:, 1],\n",
    "    \"Decision Tree\": dtree_model.predict_proba(X_test)[:, 1],\n",
    "    \"KNN\": knn_model.predict_proba(X_test)[:, 1],\n",
    "    \"XGBoost\": xgb_model.predict_proba(X_test)[:, 1]\n",
    "}\n",
    "plot_roc_curves(models_proba, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8c6ffb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Class', axis=1).values\n",
    "y_train = train_df['Class'].values\n",
    "X_test = test_df.drop('Class', axis=1).values\n",
    "y_test = test_df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5ab01257",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(\n",
    "    criterion='gini',       \n",
    "    max_depth=15,           # control tree depth to prevent overfitting\n",
    "    min_samples_split=50,   # minimum samples required to split an internal node\n",
    "    min_samples_leaf=20,    # minimum samples required to be at a leaf node\n",
    "    random_state=42         \n",
    ")\n",
    "dtree_model = dtree.fit(X_train, y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=9,        # Number of neighbors\n",
    "    weights='distance',    # 'uniform' or 'distance'\n",
    "    algorithm='auto',     # Search algorithm\n",
    "    p=2                   # 2 for Euclidean distance\n",
    ")\n",
    "knn_model = knn.fit(X_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,       # Number of boosting rounds\n",
    "    max_depth=10,            # Maximum tree depth\n",
    "    learning_rate=0.1,      # Step size shrinkage\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model = xgb.fit(X_train, y_train)\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty='l2',         # Regularization type\n",
    "    C=0.001,                # Inverse regularization strength\n",
    "    solver='liblinear',   # Solver for small datasets\n",
    "    random_state=42\n",
    ")\n",
    "log_reg_model = log_reg.fit(X_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,       # Number of trees\n",
    "    max_depth=5,           # Max depth\n",
    "    min_samples_split=50,   # Minimum samples to split\n",
    "    min_samples_leaf=50,    # Minimum samples at leaf\n",
    "    random_state=42\n",
    ")\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2135de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.82      0.72      0.77        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.91      0.86      0.88     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56848    16]\n",
      " [   27    71]]\n",
      "--- KNN ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.93      0.77      0.84        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.96      0.88      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56858     6]\n",
      " [   23    75]]\n",
      "--- XGBoost ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.93      0.77      0.84        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.96      0.88      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56858     6]\n",
      " [   23    75]]\n",
      "--- Logistic Regression ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.79      0.61      0.69        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.89      0.81      0.84     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56848    16]\n",
      " [   38    60]]\n",
      "--- Random Forest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.81      0.70      0.75        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.91      0.85      0.88     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56848    16]\n",
      " [   29    69]]\n"
     ]
    }
   ],
   "source": [
    "# Apply on all models\n",
    "test_model(\"Decision Tree\", dtree_model, X_test, y_test)\n",
    "test_model(\"KNN\", knn_model, X_test, y_test)\n",
    "test_model(\"XGBoost\", xgb_model, X_test, y_test)\n",
    "test_model(\"Logistic Regression\", log_reg_model, X_test, y_test)\n",
    "test_model(\"Random Forest\", rf_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_proba = {\n",
    "    \"Logistic Regression\": log_reg_model.predict_proba(X_test)[:, 1],\n",
    "    \"Random Forest\": rf_model.predict_proba(X_test)[:, 1],\n",
    "    \"Decision Tree\": dtree_model.predict_proba(X_test)[:, 1],\n",
    "    \"KNN\": knn_model.predict_proba(X_test)[:, 1],\n",
    "    \"XGBoost\": xgb_model.predict_proba(X_test)[:, 1]\n",
    "}\n",
    "plot_roc_curves(models_proba, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
